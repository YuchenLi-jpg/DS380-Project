---
title: "Final Project"
author: "Yuchen Li, Kede Lin, Yu Zhong, Kebei Yu"
output: html_notebook
---
### Set up
```{r}
# load necessary packages
library(tidyverse)
library(mosaic)
library(readr)
library(maps)
library(dplyr)
library(tidyverse)
library(mdsr)
library(mvtnorm)
library(factoextra)
library(ggplot2)
library(usdata)
library(ggpubr)
library(factoextra)
library(ggforce)
library(cluster)
```

### Introduction

There are hundreds of national universities that rank high globally. In this project, we intend to explore data related to the ranking of American universities. Through the data analysis of the top universities in the United States, our objectives are to to identify what conditions the university is most likely to be ranked high and build a model for future predictions. Also, we would like to perform clustering analysis to find the similar universities for references.

### Data sources and data intake

For dfRank, we select ranks of the university in 2015.

```{r}
# This contains rank and internal factors that affect rank.
dfRank <- read.csv("cwurData.csv") %>% filter(year == 2015, country == "USA")
dfCountry <- read.csv("school_and_country_table.csv")
# This is about geographic and other information for each university
dfInfo <- read.csv("world-universities.csv")
# This is demographic data by county
dfDemo <- read.csv("acs2017_county_data.csv")
```
These data was published by The Center for World University Rankings (CWUR) in 2019. And CWUR performed quantitative research to assess the quality of education, alumni employment, quality of faculty, and research performance without relying on surveys and university data submissions. Reader can download the data from the following link:
<https://www.kaggle.com/mylesoneill/world-university-rankings/download>

```{r}
MainStates <- map_data
```
This data is from __maps__ library which provide basic geographical data of main states in the US.

### Data cleaning and wrangling
```{r}
dfCountry <- dfCountry %>% mutate(country = gsub(pattern = "United States of America", replacement = "USA", country))

dfDemo <- dfDemo %>% mutate(County = gsub(pattern = " County", replacement = "", County))

head(dfCountry)
head(dfDemo)
```

```{r}
dfInfo$NAME <- tolower(dfInfo$NAME)
dfInfo$COUNTY <- tolower(dfInfo$COUNTY)
dfCountry$school_name <- tolower(dfCountry$school_name)
dfRank$institution <- tolower(dfRank$institution)
dfDemo$County <- tolower(dfDemo$County)
```
Also, we notice that the texts format is different in upper and lower cases. And we unify all the text to lower cases.

```{r}
# data set for predicting
dfML1 <- dfRank %>% select(-c("institution", "country", "year"))
# binning for top200 and others
rankData <- dfML1$world_rank
for(x in 1:length(rankData)){
  if (rankData[x] <= 200){
    dfML1$world_rank[x] <- "Top200"
  } else{
    dfML1$world_rank[x] <- "NotTop200"
  }
}
dfML1$world_rank <- as.factor(dfML1$world_rank)

head(dfML1)
```
We bin the world rank into different groups for our future exploration.

```{r}
# dataset for clustering
# add county information
dfRankCounty <- dfRank %>% inner_join(dfInfo %>% select(NAME, COUNTY, STATE), by = c("institution" = "NAME"))
# add demographic information, change the state abb to its name
dfRankCounty$STATE <- abbr2state(dfRankCounty$STATE)
dfML2 <- dfRankCounty %>% select("COUNTY", "STATE", "institution") %>% inner_join(dfDemo %>% select(everything()), by = c("COUNTY" = "County", "STATE" = "State")) %>% select(-c("COUNTY","STATE"))
```

### Classification Predicting
```{r}
glm_fits <- glm(world_rank ~ quality_of_education + alumni_employment + quality_of_faculty + patents + influence + broad_impact, data = dfML1, family = binomial)
summary(glm_fits)
```
```{r}
glm_probs <- predict(glm_fits, type = "response")
glm_probs %>% head()
```
```{r}
contrasts(dfML1$world_rank)
```
```{r}
glm_pred <- rep("NotTop200", 139)
glm_pred[glm_probs > .5] = "Top200"
glm_pred[1:100]
```
```{r}
table(glm_pred, dfML1$world_rank)
```
```{r}
mean(glm_pred == dfML1$world_rank)
```

```{r}
library(caret)

# define training control
train_control <- trainControl(method = "cv", number = 10)

# train the model on training set
model <- train(world_rank ~ quality_of_education + alumni_employment + quality_of_faculty + patents + influence + broad_impact,
               data = dfML1,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
print(model)
```

### Clustering analysis
Rank is not the only factors for future student to choose universities. We would like to consider other factors such as demographic data of the county in which the universities locate.

```{r}
internal <- dfML1 %>% select(-c("world_rank", "national_rank", "score"))
head(internal)
```

#### Perform Principle Component Analysis
```{r}
res <- internal %>% prcomp(scale = TRUE) # scale to have unit variance before analysis
str(res)
```
Inspect the loading score
```{r}
loadings <- res$rotation
score_mat <- res$x
loadings[ , 1:5]
```
Get the cumulative variance for each dim
```{r}
pve <- get_eig(res)
pve
```

Scree plot
```{r}
fviz_screeplot(res, main = "Scree Plot for the internal Dataset")
```

Cumulative PVE
```{r}
pve[1:8, ] %>% ggplot(aes(x = 1: 8, y = cumulative.variance.percent)) +
  geom_line() +
  geom_point() +
  xlab(" Principle Component") +
  ylab("Cumulative Variance Explained") +
  ggtitle("PCAs  vs Variance for internal dataset")
```
We try to find the best PCAs which can reach at leat 80% of the cumulative variance
```{r}
pve %>% filter(cumulative.variance.percent >= 80)
```

Keep the first 2 PCs
```{r}
features <- 1:2
refined <- score_mat[, features]
refined
```
Biplot
```{r}
res %>% fviz_pca_var(axes = c(1,2),
                 col.var = "contrib",
                 gradient.cols = c("#00AFBB", "#E7B800"),
                 repel = TRUE)
```
Distance Matrix
```{r}
eu_dist <- get_dist(refined, method = "euclidean")
image(as.matrix(eu_dist), main = "Euclidean Distance")
```

#### Average linkage Hierarchical Clustering
```{r}
hc_avg <- hclust(eu_dist, method = "average")
str(hc_avg)
```

```{r}
fviz_dend(hc_avg, k = 2, as.ggplot = TRUE, show_labels = FALSE, main = "Euclidean-Average")
```

Find the best number of clusters for the hierarchical clustering model.
```{r}
fviz_nbclust(internal, hcut, hc_method ="average", hc_metric = "euclidean", method ="silhouette") +
  labs(subtitle = "Silhouette method")
```

```{r}
cluster_h <- cutree(hc_avg, k = 2)
cluster_h
```
save the pairwise matrix to pdf form
```{r}
pdf(file = "pairwise.pdf")
pairs(internal, col = cluster_h) 
dev.off()
```
```{r}
internal <- cbind(internal, cluster_h)
refined <- cbind(as.data.frame(refined), cluster_h)
```

```{r}
refined %>% ggplot(aes(x = PC1, y = PC2)) +
  geom_point(col = cluster_h)
```
#### Perform kmeans clustering
```{r}
fviz_nbclust(internal, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
```
```{r}
set.seed(123)
res2 <- internal %>% kmeans(2)
sil <- silhouette(x = res2$cluster, dis = eu_dist)
sil[1:5, ]
```
```{r}
fviz_silhouette(sil)
```
```{r}
fviz_cluster(res2, data = internal,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```




